{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model_Architecture.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["* training: 70%; validation: 15%; testing: 15%\n","* Various data augmentation techniques have been applied to increase the number of training samples to reduce overfitting and improve generalization. The data augmentation techniques used are brightness, rotation, width shift, height shift, shearing, zooming, vertical flip, and horizontal flip. Also, featurewise centering, featurewise standard deviation normalization and fill mode. Before the images were supplied to other stages, they were resized to a 180 x 180.\n","\n","-\n","A pre-trained network is taken, and its classifier base is removed.\n","\n","-\n","The convolutional base of the pre-trained model is frozen.\n","\n","-\n","A new classifier is added and trained on top of the convolutional base of the pre-trained network.\n","\n","-\n","Unfreeze some layers of the convolutional base of the pre-trained network.\n","\n","-\n","Finally, both these unfrozen layers and the new classifier are jointly trained.\n","\n","https://www.sciencedirect.com/science/article/pii/S1568494620306803?casa_token=4ieLFywpVeUAAAAA:PeXRbdBj46bl9y-0SZOOE30EekImtXEMDC592HLOaOSNjKVMLBh5a9-MKeVbrATUJE8RwrSl#b26"],"metadata":{"id":"mFNNEg1zaoIy"}},{"cell_type":"markdown","source":["# Import Packages"],"metadata":{"id":"ywPmA_8SBeR7"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNjJb-mFAf-M","executionInfo":{"status":"ok","timestamp":1649368314552,"user_tz":240,"elapsed":1250,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}},"outputId":"a43e9bc1-e424-4972-a3cf-ce645a75dad1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import time \n","import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision import datasets, transforms\n","\n","from torch.optim import Optimizer\n","from torch.distributions import Bernoulli, Normal"],"metadata":{"id":"ht13Onzebw_n","executionInfo":{"status":"ok","timestamp":1649368314555,"user_tz":240,"elapsed":17,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"CbObBSbRBhkH"}},{"cell_type":"code","source":["# model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","# or any of these variants\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)\n","#model.eval()\n","\n","# num_ftrs = model.classifier.in_features\n","# model.classifier = nn.Linear(num_ftrs, 340)\n","# model.features.conv0.apply(squeeze_weights)\n","# model.classifier2 = nn.Linear(340, 1)"],"metadata":{"id":"dLnEiJypbw96","executionInfo":{"status":"ok","timestamp":1649368314555,"user_tz":240,"elapsed":15,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class TransferDenseNet121Classifier(nn.Module):\n","    def __init__(self):\n","        super(TransferDenseNet121Classifier, self).__init__()\n","        self.name = 'transfer'\n","        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","        self.num_ftrs = self.model.classifier.in_features\n","        self.model.classifier = nn.Linear(self.num_ftrs, 340)\n","        self.model.classifier2 = nn.Linear(340, 1)\n","\n","        #self.fc1 = nn.Linear(256*6*6, 32)\n","        #self.fc2 = nn.Linear(32, 1)\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.num_ftrs)\n","        x = F.relu(self.model.classifier(x))\n","        x = self.model.classifier2(x)\n","        return x"],"metadata":{"id":"-bcyNJcybw62","executionInfo":{"status":"ok","timestamp":1649368314556,"user_tz":240,"elapsed":15,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# transfer learning with densenet121 structure\n","\n","#criterion = nn.CrossEntropyLoss()\n","#optimizer = RAdam(model.parameters(), lr=0.005)\n","#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30000,60000,90000], gamma=0.3)"],"metadata":{"id":"vHFRj9nR65dQ","executionInfo":{"status":"ok","timestamp":1649368314557,"user_tz":240,"elapsed":15,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Training Code"],"metadata":{"id":"ZmNqAKylwxTy"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"HZe9PE6gY5j1","executionInfo":{"status":"ok","timestamp":1649368314558,"user_tz":240,"elapsed":15,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"outputs":[],"source":["# Generate a name for each training hyperparameter settings, which is a string consisting of all hyperparameters of a model\n","def get_model_name(name, batch_size, learning_rate, epoch):\n","    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n","                                                   batch_size,\n","                                                   learning_rate,\n","                                                   epoch)\n","    return path  #get a string consisting of all hyperparameters of a model"]},{"cell_type":"code","source":["# Compute validation loss\n","def compute_val_loss(net, loader, criterion):\n","    total_loss = 0.0\n","    for i, data in enumerate(loader, 0):  #iterate through all batches in data\n","        inputs, labels = data  \n","        outputs = net(inputs)  # make prediction\n","        loss = criterion(outputs, labels)\n","        total_loss += loss.item()\n","    loss = float(total_loss) / (i + 1)\n","    return loss"],"metadata":{"id":"rvxjHgmPxafO","executionInfo":{"status":"ok","timestamp":1649368314559,"user_tz":240,"elapsed":15,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# The train, get_accuracy and plotting code from Part 1(b) doesn't include validation set because we were just trying with a small debug set.\n","# The following codes are modified based on the code from Part 1(b) to include both training and validation sets\n","def get_accuracy(model, loader):\n","    correct = 0\n","    total = 0\n","    for imgs, labels in loader:\n","        output = model(imgs) \n","        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","    return correct / total"],"metadata":{"id":"LSgwdhKpwzZF","executionInfo":{"status":"ok","timestamp":1649368314559,"user_tz":240,"elapsed":15,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train(model, train_data, val_data, optimizer, batch_size=64, learning_rate=0.001, num_epochs=1):\n","    # Fixed PyTorch random seed for reproducible result\n","    torch.manual_seed(1000)\n","    \n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n","    criterion = nn.CrossEntropyLoss()\n","    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    train_loss = np.zeros(num_epochs)\n","    train_acc = np.zeros(num_epochs)\n","    val_loss = np.zeros(num_epochs)\n","    val_acc = np.zeros(num_epochs)\n","    \n","    \n","    # training\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        total_train_loss = 0.0\n","        i = 0\n","        \n","        for imgs, labels in iter(train_loader):\n","            optimizer.zero_grad()         # a clean up step for PyTorch\n","            out = model(imgs)             # forward pass\n","            loss = criterion(out, labels) # compute the total loss\n","            loss.backward()               # backward pass (compute parameter updates)\n","            optimizer.step()              # make the updates for each parameter\n","\n","            # Save the current training information\n","            total_train_loss += loss.item()             # compute train loss\n","            i += 1\n","        \n","        # Calculate accuracy and loss for training and validation set for this epoch\n","        train_acc[epoch] = get_accuracy(model, train_loader) # compute training accuracy \n","        val_acc[epoch] = get_accuracy(model, val_loader) # compute validation accuracy \n","        train_loss[epoch] = float(total_train_loss) / (i+1)   # compute average train loss\n","        val_loss[epoch] = compute_val_loss(model, val_loader, criterion)   # compute validation loss\n","        print((\"Epoch {}: Train acc: {}, Train loss: {} | \"+\n","               \"Val acc: {}, Val loss: {}\").format(\n","                   epoch + 1,\n","                   train_acc[epoch],\n","                   train_loss[epoch],\n","                   val_acc[epoch],\n","                   val_loss[epoch]))\n","        \n","        # Save the current model (checkpoint) to a file, every 10 epoch - always save the last epoch\n","        if (epoch % 10 == 0) or (epoch == num_epochs-1):\n","          model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n","          torch.save(model.state_dict(), model_path)\n","        \n","    print('Finished Training')\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","    \n","    # Write the train/test loss/accuracy into CSV file for plotting later\n","    epochs = np.arange(1, num_epochs + 1)\n","    model_path = get_model_name(model.name, batch_size, learning_rate, num_epochs)\n","    np.savetxt(\"{}_train_acc.csv\".format(model_path), train_acc)\n","    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n","    np.savetxt(\"{}_val_acc.csv\".format(model_path), val_acc)\n","    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n","\n","    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))"],"metadata":{"id":"kGngiO8qw15n","executionInfo":{"status":"ok","timestamp":1649368314885,"user_tz":240,"elapsed":340,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def plot_training_curve(path, epochs=1):\n","    # Load the training and validation accuracy and loss\n","    train_acc = np.loadtxt(\"{}_train_acc.csv\".format(path))\n","    val_acc = np.loadtxt(\"{}_val_acc.csv\".format(path))\n","    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n","    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n","    \n","    plt.title(\"Train vs Validation Accuracy\")\n","    plt.plot(range(1,epochs+1), train_acc, label=\"Train\")\n","    plt.plot(range(1,epochs+1), val_acc, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","    \n","    plt.title(\"Train vs Validation Loss\")\n","    plt.plot(range(1,epochs+1), train_loss, label=\"Train\")\n","    plt.plot(range(1,epochs+1), val_loss, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    \n","    plt.show()"],"metadata":{"id":"2SzWaL-cw5Ew","executionInfo":{"status":"ok","timestamp":1649368314885,"user_tz":240,"elapsed":5,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Optimizers\n","\n","https://github.com/nicklashansen/neural-net-optimization/blob/master/optimizers.py"],"metadata":{"id":"xcA_M5MN7dfC"}},{"cell_type":"code","source":["class SGD(Optimizer):\n","    \"\"\"\n","    Stochastic gradient descent. Also includes implementations of momentum,\n","    Nesterov's momentum, L2 regularization, SGDW and Learning Rate Dropout.\n","    \"\"\"\n","    def __init__(self, params, lr, mu=0, nesterov=False, weight_decay=0, lrd=1):\n","        defaults = {'lr': lr, 'mu': mu, 'nesterov': nesterov, 'weight_decay': weight_decay, 'lrd': lrd}\n","        super(SGD, self).__init__(params, defaults)\n","\n","    def step(self):\n","        \"\"\"\n","        Performs a single optimization step.\n","        \"\"\"\n","        for group in self.param_groups:\n","\n","            lr = group['lr']\n","            mu = group['mu']\n","            nesterov = group['nesterov']\n","            weight_decay = group['weight_decay']\n","            lrd_bernoulli = Bernoulli(probs=group['lrd'])\n","\n","            if mu != 0 and 'v' not in group:\n","                group['v'] = []\n","                if nesterov:\n","                    group['theta'] = []\n","                for param in group['params']:\n","                    group['v'].append(torch.zeros_like(param))\n","                    if nesterov:\n","                        theta_param = torch.ones_like(param).mul_(param.data)\n","                        group['theta'].append(theta_param)\n","\n","            for idx, param in enumerate(group['params']):\n","                param.grad.data -= weight_decay * param.data\n","                lrd_mask = lrd_bernoulli.sample(param.size()).to(param.device)\n","\n","                if mu != 0:\n","                    v = group['v'][idx]\n","                    v = mu * v - lr * param.grad.data\n","                    group['v'][idx] = v\n","\n","                    if nesterov:\n","                        group['theta'][idx] += lrd_mask * v\n","                        param.data = group['theta'][idx] + mu * v\n","\n","                    else:\n","                        param.data += lrd_mask * v\n","\n","                else:\n","                    param.data -= lrd_mask * lr * param.grad.data\n","\n","\n","class Adam(Optimizer):\n","    \"\"\"\n","    Adam as proposed by https://arxiv.org/abs/1412.6980.\n","    Also includes a number of proposed extensions to the the Adam algorithm,\n","    such as Nadam, L2 regularization, AdamW, RAdam and Learning Rate Dropout.\n","    \"\"\"\n","    def __init__(self, params, lr, beta1=0.9, beta2=0.999, nesterov=False, l2_reg=0, weight_decay=0, rectified=False, lrd=1, eps=1e-8):\n","        defaults = {'lr': lr, 'beta1': beta1, 'beta2': beta2, 'nesterov': nesterov, 'l2_reg': l2_reg,\n","                    'weight_decay': weight_decay, 'rectified': rectified, 'lrd': lrd, 'eps': eps}\n","        super(Adam, self).__init__(params, defaults)\n","\n","    def step(self):\n","        \"\"\"\n","        Performs a single optimization step.\n","        \"\"\"\n","        for group in self.param_groups:\n","\n","            lr = group['lr']\n","            beta1 = group['beta1']\n","            beta2 = group['beta2']\n","            nesterov = group['nesterov']\n","            l2_reg = group['l2_reg']\n","            weight_decay = group['weight_decay']\n","            rectified = group['rectified']\n","            lrd_bernoulli = Bernoulli(probs=group['lrd'])\n","            eps = group['eps']\n","\n","            if 'm' not in group and 'v' not in group:\n","                group['m'] = []\n","                group['v'] = []\n","                group['t'] = 1\n","                if nesterov:\n","                    group['prev_grad'] = []\n","                for param in group['params']:\n","                    group['m'].append(torch.zeros_like(param))\n","                    group['v'].append(torch.zeros_like(param))\n","                    if nesterov:\n","                        group['prev_grad'].append(torch.zeros_like(param))\n","\n","            for idx, param in enumerate(group['params']):\n","                if l2_reg:\n","                    param.grad.data += l2_reg * param.data\n","\n","                if nesterov:\n","                    grad = group['prev_grad'][idx]\n","                else:\n","                    grad = param.grad.data\n","\n","                lrd_mask = lrd_bernoulli.sample(param.size()).to(param.device)\n","\n","                m = group['m'][idx]\n","                v = group['v'][idx]\n","                t = group['t']\n","                m = beta1 * m + (1 - beta1) * grad\n","                v = beta2 * v + (1 - beta2) * grad**2\n","                m_hat = m / (1 - beta1**t)\n","                v_hat = v / (1 - beta2**t)\n","\n","                if nesterov:\n","                    group['prev_grad'][idx] = param.grad.data\n","\n","                if rectified:\n","                    rho_inf = 2 / (1 - beta2) - 1\n","                    rho = rho_inf - 2 * t * beta2**t / (1 - beta2**t)\n","                    if rho >= 5:\n","                        numerator = (1 - beta2**t) * (rho - 4) * (rho - 2) * rho_inf\n","                        denominator = (rho_inf - 4) * (rho_inf - 2) * rho\n","                        r = np.sqrt(numerator / denominator)\n","                        param.data += - lrd_mask * lr * r * m_hat / (torch.sqrt(v) + eps)\n","                    else:\n","                        param.data += - lrd_mask * lr * m_hat\n","                else:\n","                    param.data += - lrd_mask * lr * m_hat / (torch.sqrt(v_hat) + eps)\n","\n","                if weight_decay:\n","                    param.data -= weight_decay * param.data\n","\n","                group['m'][idx] = m\n","                group['v'][idx] = v\n","\n","            group['t'] += 1\n","\n","\n","class RMSProp(Adam):\n","    \"\"\"\n","    RMSprop as proposed by http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.\n","    Note that this implementation, unlike the original RMSprop, uses bias-corrected moments.\n","    \"\"\"\n","    def __init__(self, params, lr, beta2):\n","        super(RMSProp, self).__init__(params, lr, beta2=beta2, beta1=0)\n","\n","\n","class Lookahead(Optimizer):\n","    \"\"\"\n","    Lookahead Optimization as proposed by https://arxiv.org/abs/1907.08610.\n","    This is a wrapper class that can be applied to an instantiated optimizer.\n","    \"\"\"\n","    def __init__(self, optimizer, k=5, alpha=0.5):\n","        self.optimizer = optimizer\n","        self.k = k\n","        self.alpha = alpha\n","        self.param_groups = optimizer.param_groups\n","\n","        self.counter = 0\n","        for group in optimizer.param_groups:\n","            group['phi'] = []\n","            for param in group['params']:\n","                phi_param = torch.ones_like(param).mul_(param.data)\n","                group['phi'].append(phi_param)\n","\n","    def step(self):\n","        if self.counter == self.k:\n","            for group_idx, group in enumerate(self.param_groups):\n","                for idx, _ in enumerate(group['phi']):\n","                    theta = self.optimizer.param_groups[group_idx]['params'][idx].data\n","                    group['phi'][idx] = group['phi'][idx] + self.alpha * (theta - group['phi'][idx])\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            self.optimizer.step()\n","\n","\n","class GradientNoise(Optimizer):\n","    \"\"\"\n","    Gradient Noise as proposed by https://arxiv.org/abs/1511.06807.\n","    This is a wrapper class that can be applied to an instantiated optimizer.\n","    \"\"\"\n","    def __init__(self, optimizer, eta=0.3, gamma=0.55):\n","        self.optimizer = optimizer\n","        self.eta = eta\n","        self.gamma = gamma\n","        self.t = 0\n","        self.param_groups = optimizer.param_groups\n","\n","    def step(self):\n","        normal = torch.empty(1).normal_(mean=0, std=np.sqrt(self.eta/((1+self.t)**self.gamma)))\\\n","            .to(self.optimizer.param_groups[0]['params'][0].device)\n","        for group_idx, group in enumerate(self.param_groups):\n","            for idx, param in enumerate(group['params']):\n","                self.optimizer.param_groups[group_idx]['params'][idx].grad.data += normal\n","                self.optimizer.step()\n","                self.t += 1\n","\n","\n","class GradientDropout(Optimizer):\n","    \"\"\"\n","    Gradient dropout as proposed by https://arxiv.org/abs/1912.00144.\n","    This is a wrapper class that can be applied to an instantiated optimizer.\n","    Note that this method does not improve optimization significantly and\n","    is only here for comparison to Learning Rate Dropout.\n","    \"\"\"\n","    def __init__(self, optimizer, grad_retain=0.9):\n","        self.optimizer = optimizer\n","        self.grad_retain = grad_retain\n","        self.grad_bernoulli = Bernoulli(probs=grad_retain)\n","        self.param_groups = optimizer.param_groups\n","\n","    def step(self):\n","        for group_idx, group in enumerate(self.param_groups):\n","            for idx, param in enumerate(group['params']):\n","                grad_mask = self.grad_bernoulli.sample(param.size()).to(param.device)\n","                self.optimizer.param_groups[group_idx]['params'][idx].grad.data *= grad_mask\n","                self.optimizer.step()"],"metadata":{"id":"BaU6j6II7e7P","executionInfo":{"status":"ok","timestamp":1649368315190,"user_tz":240,"elapsed":309,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Actual Training"],"metadata":{"id":"_iXf7YZ4-l9m"}},{"cell_type":"code","source":["tensor_data_path = \"/content/drive/My Drive/MIE424/Exploring-Optimization-in-Deep-Learning-with-Various-Algorithms/Data/Tensor/\""],"metadata":{"id":"WOETfIXfAxCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testdataset = torchvision.datasets.DatasetFolder(tensor_data_path, loader=torch.load, extensions=('.pt'))"],"metadata":{"id":"Ap3VLIJgAttT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the computed train and validation features\n","train_features = torch.load('Alexnet_train_feature')\n","val_features = torch.load('Alexnet_val_feature')"],"metadata":{"id":"g-j5Ebyp-hQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train with default batch size and learning rate\n","model = TransferDenseNet121Classifier()\n","optimizer = Adam()\n","train(model, train_features, val_features, optimizer, batch_size=32, learning_rate=0.01, num_epochs=15)"],"metadata":{"id":"25JC9r23-GUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting is used for choosing hyperparameters\n","model_path = get_model_name('transfer', batch_size=32, learning_rate=0.01, epoch=15)\n","plot_training_curve(model_path, epochs=15)"],"metadata":{"id":"a36hxCQ3-vk_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Useful Links:  \n","https://www.kaggle.com/code/jaeboklee/pytorch-transfer-learning-with-densenet/notebook  \n","https://www.upgrad.com/blog/basic-cnn-architecture/  \n","https://towardsdatascience.com/a-bunch-of-tips-and-tricks-for-training-deep-neural-networks-3ca24c31ddc8  \n","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8189817/  \n","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8300985/"],"metadata":{"id":"PwovqQNdDCA0"}}]}