{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optimizers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN231SL6K+tmqx+gwoEN7t7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import numpy as np\n","from torch.optim import Optimizer\n","from torch.distributions import Bernoulli, Normal"],"metadata":{"id":"JXaYrn34Yeuh","executionInfo":{"status":"ok","timestamp":1649374558522,"user_tz":240,"elapsed":7216,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["https://github.com/nicklashansen/neural-net-optimization/blob/master/optimizers.py"],"metadata":{"id":"W4Bh0aDBYk_w"}},{"cell_type":"code","source":["class SGD(Optimizer):\n","    \"\"\"\n","    Stochastic gradient descent. Also includes implementations of momentum,\n","    Nesterov's momentum, L2 regularization, SGDW and Learning Rate Dropout.\n","    \"\"\"\n","    def __init__(self, params, lr, mu=0, nesterov=False, weight_decay=0, lrd=1):\n","        defaults = {'lr': lr, 'mu': mu, 'nesterov': nesterov, 'weight_decay': weight_decay, 'lrd': lrd}\n","        super(SGD, self).__init__(params, defaults)\n","\n","    def step(self):\n","        \"\"\"\n","        Performs a single optimization step.\n","        \"\"\"\n","        for group in self.param_groups:\n","\n","            lr = group['lr']\n","            mu = group['mu']\n","            nesterov = group['nesterov']\n","            weight_decay = group['weight_decay']\n","            lrd_bernoulli = Bernoulli(probs=group['lrd'])\n","\n","            if mu != 0 and 'v' not in group:\n","                group['v'] = []\n","                if nesterov:\n","                    group['theta'] = []\n","                for param in group['params']:\n","                    group['v'].append(torch.zeros_like(param))\n","                    if nesterov:\n","                        theta_param = torch.ones_like(param).mul_(param.data)\n","                        group['theta'].append(theta_param)\n","\n","            for idx, param in enumerate(group['params']):\n","                param.grad.data -= weight_decay * param.data\n","                lrd_mask = lrd_bernoulli.sample(param.size()).to(param.device)\n","\n","                if mu != 0:\n","                    v = group['v'][idx]\n","                    v = mu * v - lr * param.grad.data\n","                    group['v'][idx] = v\n","\n","                    if nesterov:\n","                        group['theta'][idx] += lrd_mask * v\n","                        param.data = group['theta'][idx] + mu * v\n","\n","                    else:\n","                        param.data += lrd_mask * v\n","\n","                else:\n","                    param.data -= lrd_mask * lr * param.grad.data\n","\n","\n","class Adam(Optimizer):\n","    \"\"\"\n","    Adam as proposed by https://arxiv.org/abs/1412.6980.\n","    Also includes a number of proposed extensions to the the Adam algorithm,\n","    such as Nadam, L2 regularization, AdamW, RAdam and Learning Rate Dropout.\n","    \"\"\"\n","    def __init__(self, params, lr, beta1=0.9, beta2=0.999, nesterov=False, l2_reg=0, weight_decay=0, rectified=False, lrd=1, eps=1e-8):\n","        defaults = {'lr': lr, 'beta1': beta1, 'beta2': beta2, 'nesterov': nesterov, 'l2_reg': l2_reg,\n","                    'weight_decay': weight_decay, 'rectified': rectified, 'lrd': lrd, 'eps': eps}\n","        super(Adam, self).__init__(params, defaults)\n","\n","    def step(self):\n","        \"\"\"\n","        Performs a single optimization step.\n","        \"\"\"\n","        for group in self.param_groups:\n","\n","            lr = group['lr']\n","            beta1 = group['beta1']\n","            beta2 = group['beta2']\n","            nesterov = group['nesterov']\n","            l2_reg = group['l2_reg']\n","            weight_decay = group['weight_decay']\n","            rectified = group['rectified']\n","            lrd_bernoulli = Bernoulli(probs=group['lrd'])\n","            eps = group['eps']\n","\n","            if 'm' not in group and 'v' not in group:\n","                group['m'] = []\n","                group['v'] = []\n","                group['t'] = 1\n","                if nesterov:\n","                    group['prev_grad'] = []\n","                for param in group['params']:\n","                    group['m'].append(torch.zeros_like(param))\n","                    group['v'].append(torch.zeros_like(param))\n","                    if nesterov:\n","                        group['prev_grad'].append(torch.zeros_like(param))\n","\n","            for idx, param in enumerate(group['params']):\n","                if l2_reg:\n","                    param.grad.data += l2_reg * param.data\n","\n","                if nesterov:\n","                    grad = group['prev_grad'][idx]\n","                else:\n","                    grad = param.grad.data\n","\n","                lrd_mask = lrd_bernoulli.sample(param.size()).to(param.device)\n","\n","                m = group['m'][idx]\n","                v = group['v'][idx]\n","                t = group['t']\n","                m = beta1 * m + (1 - beta1) * grad\n","                v = beta2 * v + (1 - beta2) * grad**2\n","                m_hat = m / (1 - beta1**t)\n","                v_hat = v / (1 - beta2**t)\n","\n","                if nesterov:\n","                    group['prev_grad'][idx] = param.grad.data\n","\n","                if rectified:\n","                    rho_inf = 2 / (1 - beta2) - 1\n","                    rho = rho_inf - 2 * t * beta2**t / (1 - beta2**t)\n","                    if rho >= 5:\n","                        numerator = (1 - beta2**t) * (rho - 4) * (rho - 2) * rho_inf\n","                        denominator = (rho_inf - 4) * (rho_inf - 2) * rho\n","                        r = np.sqrt(numerator / denominator)\n","                        param.data += - lrd_mask * lr * r * m_hat / (torch.sqrt(v) + eps)\n","                    else:\n","                        param.data += - lrd_mask * lr * m_hat\n","                else:\n","                    param.data += - lrd_mask * lr * m_hat / (torch.sqrt(v_hat) + eps)\n","\n","                if weight_decay:\n","                    param.data -= weight_decay * param.data\n","\n","                group['m'][idx] = m\n","                group['v'][idx] = v\n","\n","            group['t'] += 1\n","\n","\n","class RMSProp(Adam):\n","    \"\"\"\n","    RMSprop as proposed by http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.\n","    Note that this implementation, unlike the original RMSprop, uses bias-corrected moments.\n","    \"\"\"\n","    def __init__(self, params, lr, beta2):\n","        super(RMSProp, self).__init__(params, lr, beta2=beta2, beta1=0)\n","\n","\n","class Lookahead(Optimizer):\n","    \"\"\"\n","    Lookahead Optimization as proposed by https://arxiv.org/abs/1907.08610.\n","    This is a wrapper class that can be applied to an instantiated optimizer.\n","    \"\"\"\n","    def __init__(self, optimizer, k=5, alpha=0.5):\n","        self.optimizer = optimizer\n","        self.k = k\n","        self.alpha = alpha\n","        self.param_groups = optimizer.param_groups\n","\n","        self.counter = 0\n","        for group in optimizer.param_groups:\n","            group['phi'] = []\n","            for param in group['params']:\n","                phi_param = torch.ones_like(param).mul_(param.data)\n","                group['phi'].append(phi_param)\n","\n","    def step(self):\n","        if self.counter == self.k:\n","            for group_idx, group in enumerate(self.param_groups):\n","                for idx, _ in enumerate(group['phi']):\n","                    theta = self.optimizer.param_groups[group_idx]['params'][idx].data\n","                    group['phi'][idx] = group['phi'][idx] + self.alpha * (theta - group['phi'][idx])\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            self.optimizer.step()\n","\n","\n","class GradientNoise(Optimizer):\n","    \"\"\"\n","    Gradient Noise as proposed by https://arxiv.org/abs/1511.06807.\n","    This is a wrapper class that can be applied to an instantiated optimizer.\n","    \"\"\"\n","    def __init__(self, optimizer, eta=0.3, gamma=0.55):\n","        self.optimizer = optimizer\n","        self.eta = eta\n","        self.gamma = gamma\n","        self.t = 0\n","        self.param_groups = optimizer.param_groups\n","\n","    def step(self):\n","        normal = torch.empty(1).normal_(mean=0, std=np.sqrt(self.eta/((1+self.t)**self.gamma)))\\\n","            .to(self.optimizer.param_groups[0]['params'][0].device)\n","        for group_idx, group in enumerate(self.param_groups):\n","            for idx, param in enumerate(group['params']):\n","                self.optimizer.param_groups[group_idx]['params'][idx].grad.data += normal\n","                self.optimizer.step()\n","                self.t += 1\n","\n","\n","class GradientDropout(Optimizer):\n","    \"\"\"\n","    Gradient dropout as proposed by https://arxiv.org/abs/1912.00144.\n","    This is a wrapper class that can be applied to an instantiated optimizer.\n","    Note that this method does not improve optimization significantly and\n","    is only here for comparison to Learning Rate Dropout.\n","    \"\"\"\n","    def __init__(self, optimizer, grad_retain=0.9):\n","        self.optimizer = optimizer\n","        self.grad_retain = grad_retain\n","        self.grad_bernoulli = Bernoulli(probs=grad_retain)\n","        self.param_groups = optimizer.param_groups\n","\n","    def step(self):\n","        for group_idx, group in enumerate(self.param_groups):\n","            for idx, param in enumerate(group['params']):\n","                grad_mask = self.grad_bernoulli.sample(param.size()).to(param.device)\n","                self.optimizer.param_groups[group_idx]['params'][idx].grad.data *= grad_mask\n","                self.optimizer.step()"],"metadata":{"id":"_nJRn5CMYahu","executionInfo":{"status":"ok","timestamp":1649375557124,"user_tz":240,"elapsed":374,"user":{"displayName":"Nancy Li","userId":"12562624317552008542"}}},"execution_count":6,"outputs":[]}]}